\section{Introduction}
Decision making is increasingly data-driven and this has catalyzed an immense research interest in large scale analytical query processing.
However, large datasets can be prone to error \cite{Gartner}, and data cleaning has been studied to mitigate query error on dirty data \cite{dasu2003exploratory, mayfield2010eracer, openrefine, wrangler, DBLP:conf/sigmod/DallachiesaEEEIOT13, DBLP:conf/pervasive/JefferyAFHW06}.
%When querying dirty data, the speed of the query processing engine does not matter if the results are erroneous.
In many cases, data cleaning is one of the most expensive and crucial steps in a data analysis workflow.
Cleaning and transforming data is the often first step \emph{before} any analysis such as machine learning can even begin.

A typical use case often requires a sequence of data cleaning operations 
such as imputation, string formatting, \emph{and} record linkage.
Some of these operations are routine and can be even abstracted into a data cleaning library.
In the SampleClean project \cite{wang1999sample}, we built a distributed query engine that combined approximate analytics query processing 
and a library of data cleaning operations.
One of the goals of this probject was to build a declarative interface for data cleaning, where users specify what types of errors
occured in their dataset (eg. duplicates and missing values) and the SampleClean framework would optimally execute the necessary libary routines.
We call these sequences of data cleaning operations \emph{pipelines}, and in this paper, we aim to study the robustness of these pipelines.

The key difficulty in reasoning about \emph{pipelines} is that all of thse operations are in effect heuristics with some precision and recall characteristics.
Pipelining heuristic operations is by nature an uncertain process; drawing a stark contrast to the traditional query optimization setting where correctness can be formalized in terms of confluence or serializability [cite]. 
We found that pipelines were often very brittle and small changes in the order-of-operations could have very significant impact on results.
To get reliable results we had to specify these operations and their ordering very precisely, which is in opposition to the goal of building a declarative data cleaning interface.

We focus our study on an important subclass of data cleaning pipeline problems: entity resolution (ER) \cite{DBLP:journals/pvldb/KopckeTR10, conf/dmkd/MongeE97, conf/sigmod/WhangMKTG09, conf/acl/FinkelM08, conf/sigmod/WangLF12, Fellegi1969, conf/sigmod/ArasuGK10, DBLP:journals/tkde/ElmagarmidIV07, journals/tkde/Christen11}. 
In these problems, for every record we want to find a single canonical mapping between the records and the real world.
Formally, every record in the clean relation $R_c$ there is a one-to-one mapping with some real world entity.
Typically, ER algorithms build a graph of linked candidate records and apply a graph clustering algorithm to resolve groups of records that refer to the same entity.
For scalability, these graphs are often made very sparse.

However, there are two key recent results that suggest changing the way in which we approach ER: (1) the combination of sampling and data cleaning \cite{wang1999sample}, and (2) improved parallel graph segmentation algorithms [cite]. 
In SampleClean, we showed that applying data cleaning to a sample of data was sufficient to answer many non-selective aggregate queries with high accuracy. We have also been working on parallel correlation clustering graph algorithms which can cluster graphs with millions of vertices in seconds [cite].
We see these two developments as an opportunity to address the ER problem with much larger, denser entity graphs, and we use the dense graphs as a tool for increased robustness.

We leverage these recent results to take a broader view of entity resolution.
ER in its typical formulation requires merging duplicate records that map to the same entity.
To this task, we add the following operations fixing the attributes of the record so they accurately reflect the entity, and accurately filling in missing attributes.
Linking back to our discussions of \emph{pipelines}, each ER task requires a sequence of duplication and attribute correction operations.
Our key insight is that we can represent the entire end-to-end task as a single graph, which also encapsulates the lineage of a tuple through the pipeline--not just the deduplication.
This end-to-end model can lead to more robust detection of missing edges in the graph to to heurstic algorithms.

However, one problem with this model is that it could lead to many false positive edges.
We address this through randomization.
We execute the pipeline in random permutations of the operations.
Each execution has its own graphical model, and we take the union of the graphs.
Edges and vertices that exists in multiple graphs are weighted more highly; which is valuable information when we want to cluster this graph.

The intuition for robustness is clear, by picking a single sequence before hand, we are putting our full faith that this pipeline is the best one.
Instead, we propose a pipeline optimization algorithm called Ensemble of Sampled Pipelines (ESP), which is a Monte-Carlo algorithm that in effect hedges its bets on multiple randomly sampled pipelines.
However, one challenge is that all of these random samples have to be executed completely before results can be presented to users.
We propose a streaming variant of the correlation clustering algorithm that allows users to see the results update as more random pipelines are executed.

Our main contributions are:
\begin{itemize}
\item An API to specify entity resolution subtasks including duplication and attribute correction, and pipeline these operations.
\item A framework for turning the execution of a pipeline into a single graphical model which we call a \emph{proposal}.
\item A randomized sampling from the set of pipelines (which can be constrained by user ``hints" about ordering) which takes a union of the proposal graphs.
\item A parallel, streaming correlation clustering approach to allow results to update as more executions of the randomized pipelines complete. 
\end{itemize}









