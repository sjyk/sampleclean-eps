\section{Introduction}
Decision making is increasingly data-driven and this has catalyzed an immense research interest in large scale analytical query processing.
However, large datasets can be prone to error \cite{Gartner}, and data cleaning has been studied to mitigate query error on dirty data \cite{dasu2003exploratory, mayfield2010eracer, openrefine, wrangler, DBLP:conf/sigmod/DallachiesaEEEIOT13, DBLP:conf/pervasive/JefferyAFHW06}.
%When querying dirty data, the speed of the query processing engine does not matter if the results are erroneous.
In many cases, data cleaning is one of the most crucial steps in a data analysis workflow.
Cleaning and transforming data is the often first step \emph{before} any analysis such as machine learning can even begin.

A typical application often requires a sequence of data cleaning operations 
such as imputation, string formatting, \emph{and} record linkage.
Some of these operations are routine and can be even abstracted into a data cleaning library.
In the SampleClean project \cite{wang1999sample}, we built a distributed query engine that combined approximate analytics query processing 
and a library of data cleaning operations.
One of the goals of this project was to build a declarative interface for data cleaning, where users specify what types of errors
occured in their dataset (eg. duplicates and missing values) and the SampleClean framework would optimally execute the necessary libary routines.
We call these sequences of data cleaning operations \emph{pipelines}.

The key difficulty in reasoning about \emph{pipelines} is that all of thse operations are in effect heuristics with some precision and recall characteristics.
Pipelining heuristic operations is by nature an uncertain process; drawing a stark contrast to the traditional query optimization setting where correctness can be formalized in terms of confluence or serializability. 
We found that pipelines were often very brittle and small changes in the order-of-operations could have very significant impact on results.
To get reliable results we had to specify these operations and their ordering very precisely, which is in opposition to the goal of building a declarative data cleaning interface.

In this paper, we aim to study the robustness of these pipelines and techniques which can allow for more reliable execution.
We focus our study on an important subclass of data cleaning pipeline problems: entity resolution (ER) \cite{DBLP:journals/pvldb/KopckeTR10, conf/dmkd/MongeE97, conf/sigmod/WhangMKTG09, conf/acl/FinkelM08, conf/sigmod/WangLF12, Fellegi1969, conf/sigmod/ArasuGK10, DBLP:journals/tkde/ElmagarmidIV07, journals/tkde/Christen11}. 
In these problems, for every record we want to find a single canonical mapping between the records and the real world.
Formally, every record in the clean relation $R_c$ there is a one-to-one mapping with some real world entity.
Typically, ER algorithms build a graph of linked candidate records (\emph{entity graph}) and apply a graph clustering algorithm to resolve groups of records that refer to the same entity.

In a traditional ER workflow, all pairs of records are heuristically matched with some measure of similarity (eg. Jaccard similarity for string-valued attributes).
When applying heuristics to construct an entity graph, there are two types of errors: false-positives and false-negatives. 
In the false-positive case, an edge linking two records is spuriously added. 
In the false-negative case, a necessary edge is missing. 
For scalability, these graphs are often made very sparse.
A dense graph could may require $O(N^2)$ space making it impractical for a large number of records.
To cope with this problem heuristics are usually designed to be conservative avoiding the false-positive case.
This makes the graphs very sparse and also allows flexibility to apply coarser-grained clustering.

With this challenge in mind, there are two key recent results that present an opportunity to change the way in which we approach ER: (1) the combination of sampling and data cleaning \cite{wang1999sample}, and (2) improved parallel graph segmentation algorithms [cite]. 
In SampleClean, we showed that applying data cleaning to a sample of data was sufficient to answer many non-selective aggregate queries with high accuracy. We have also been working on parallel correlation clustering graph algorithms which can cluster graphs with millions of vertices in seconds [cite].
With the conjunction of these two techniques, we can support much larger and denser entity graphs.

We first take a broader view of entity resolution.
ER in its typical formulation requires merging duplicate records that map to the same entity; which in reality is only the last step of a typical data cleaning workflow.
To this task effective, we often have to start by fixing the attributes of the record so they accurately reflect the entity, and accurately filling in missing attributes.
Linking back to our discussions of \emph{pipelines}, each ER task requires a sequence of duplication and attribute correction operations.
Every record then has a history of changes as it progresses through the pipeline and contribution (1) of this paper is exploring the benefits of exposing this history to data cleaning operators.
For example, perhaps an earlier version of a tuple is easier to link to a later version of another.

Contribution (2) addresses the problem of the heuristic nature of the pipeline.
Instead of picking a set optimal order before execution, we execute random permutations of the pipeline.
We use these random executions to find linked tuples that stay linked across different permutations of the pipeline.
However, one challenge is that all of these random samples have to be executed completely before results can be presented to users.
Contribution (3) of this works is a streaming variant of the correlation clustering algorithm that allows users to see the results update as more random pipelines are executed.

We call this entire approach ESP (Ensemble of Sample Pipelines), and we evaluate this on real and synthetic datasets.
We also implement this framework in a distributed setting with Spark 1.1 and GraphX.
On three ER datasets, a product dataset, a restaurant dataset, and a dataset of papers fro MS Academic Search, 
our results suggest that ESP outperforms the graph-based algorithm in\cite{wang2012crowder}, is also competitive with single best pipeline, and outperforms the worst pipeline.
While, not always better than the optimal choice of pipeline single ESP ensures that the result is never too bad.









