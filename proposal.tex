\section{Proposals}
We first look at subproblem relevant to the pipeline optimization; how do we represent the relationships 
between tuples in these entity resolution tasks.
We call this intermediate data structure a proposal since metaphorically the data cleaning operations propose changes.
The execution engine then takes this proposal and formulates an actionable execution plan based on the proposal actually apply the changes.

The solution will be to break apart the ER operators in the previous section and represent their operations
as a graph.
In this section, we describe this procedure for a single given pipeline.
Then, we discuss how to merge different proposals.

\subsection{Building Proposals}
We re-define entity resolution in terms of what we call \emph{proposal operations}.
That is instead of making a change, the operation proposes a candidate change.
By exposing this intermediate state rather than immediate applying changes, we can relax some of the assumptions we made
in the previous section when formalizing entity resolution.

First, $\delta$ operations need not execute one-to-one merges.
We can propose a set of candidate merges e.g. row1 one is linked to row2 and row3.
Similarly, with $\kappa$ operations we can propose a set of possible transformations e.g. if my attribute is a comma separated string value, the relevant data is either in 1st or 2nd field.
We can defer enforcing correctness to when the proposal is actually executed.
One way to think about this approach is that we are building a probabilistic database on a deterministic database and lazily marginalizing the distributions at execution time [cite].

\begin{definition} Entity Resolution Proposals. 
Proposal operations are functional operations on a weighted graph $G=(V,E,W(e),M(v))$ with edge weights $W(e)$ and a multiplicity function $M(v)$ for each vertex. 
Records in the database are represented as vertices, and proposed corrections/merges are represented as edges.
\begin{itemize}
\item Correct $\kappa(G,a_{1})$ For every $v \in V$, let $v'$ be the corrected version of $v$. We add $v'$ to $V$ if it does not exist and we draw an edge between $v$ and $v'$ with weight $w$ where a larger $w$ indicates increased confidence. If it already exists, we increment the multiplicty counter for $v'$. If $v$ has multiple possible corrected versions, repeat for all.
\item Deduplication $\delta(G)$ For every $v \in V$, draw an edge starting at $v$ to represent merging two records with weight $w$ where a larger $w$ indicates increased confidence. If it has multiple possible merges, draw edges for all.
\end{itemize}
%Like before, we constrain that $\kappa$ and $\delta$ are consistent; that is given two records that are identical on the projection their operation is identical.
\end{definition}

The end of a given pipeline will be a graph $G$. 
This graph structure is a generalization of the graph structure discussed in entity resolution [cite]; i.e. the graph in an open world model.

\subsection{Proposal API}
\reminder{TODO}

\subsection{Benefits from this Data Structure}
We list out some of the benefits of this data structure and what we can do with proposals that was not previously possible in pipeline of ``off-the-shelf" entity resolution components. First, proposals give a natural way to represent uncertainty using weighted edges. They also subsume deterministic algorithms as we can represent a tree of merges with uniform weights. It is true that internally, many ER algorithms use a similar construction, however, by building a single, unfied data structure to represent both $\kappa$ and $\delta$ operations, we can propagate the data structure through a pipeline of reusable components. 

The second benefit is a representation of history. 
By representing the $\kappa$ operations in the data structure, we do not lose history when we apply an operation.
For example, an operation might be correct for 80\% of records and give incorrect results for the last 20\%, and the data structure gives us a natural way to interface with previously correct results.

The practical implications of this structure are interesting. 
For example, in ER pipelines, we often have to make many decisions up front such as what string tokenizer to use, how to threshold heuristics, or how dates should be formatted.
In this model, with the proposal structure, we argue why not try all of them.
Obviously there is a trade-off space between ER accuracy and performance, and we will discuss that more in the next section.

\subsection{Proposal Overhead}
The proposal framework introduces overhead as the graph $G$ is much larger in terms of both edges and vertices than typically considered in graph based ER tasks. 
This warrants new algorithms to process this larger graph.
If $p$ is the number of steps in the pipeline, and $k$ is the average number of candidates $O(p^2k^2+V^2)$.
\reminder{TODO}

\subsection{Algebra Over Proposals}
Given two different proposals $G_1$ and $G_2$, we can define a merge operations to make them a single proposal.
\reminder{TODO}

